---
title: "Week 7"
---

<div style="text-align: justify">

## Summary of Lecture 7

```{r echo=FALSE,cache=FALSE, fig.align='center', fig.cap="The feature relations map learning (FRML) chart for hyperspectral image classification. Source: mdpi.com/2072-4292/12/18/2956"}
knitr::include_graphics('https://www.mdpi.com/remotesensing/remotesensing-12-02956/article_deploy/html/images/remotesensing-12-02956-g001.png')
```

Due to the synoptic view nature in remote sensing images, the remote sensing images produces are in map-like format, making it a viable source of gathering effective land cover information. However, just the raw image and identification with the naked eye is not reliable and accurate. Therefore, basic image classification is important to categorize all pixels in a digital image into one of several classes. Image classification is regarded as one of the most essential part in digital image analysis as it allows allocation of semantic labels to capture image. It is useful for research, and policy making purposes as it provides better understanding and details to the content within the image. The image above is a type of image processing using a technique of **Feature Relations Map Learning** for hyperspectral image, it automatically enhance the separability of different objects in an image [@dou2020]. While using a segmented feature relations map (SFRM), it reflects the relations between spectral features through a normalized difference index (NDI), and it can then learn new features from SFRM using a CNN-based feature extractor [@dou2020]. Conventionally, there are three types of image classification, including

1.  ***Manual Classification***

2.  ***Pixel-based classification***

    1.  Supervised image classification

    2.  Unsupervised image classification

3.  ***Feature or object-based image classification***

### **Pixel-based Classification**

In pixel-based classification, there are two types of image classifications,including **Unsupervised** and **Supervised** classification, where unsupervised classification is calculated by the software supervised is mainly a human-guided classification. The supervised image classification creates outcomes that are based on the software analysis without giving it sample classes. On the other hand, unsupervised classification creates outcome that is aided by providing sample pixels in an image that are representative of specific classes and then direct the image processing software to use these training sites as references for the classification of all other pixels in the image. The figures below shows the two types of image classification technique used on the same image, producing approximately similar results, yet the details are quite different.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Supervised and unsupervised classification on an image. Source: https://beekangsi.com/the-pros-and-cons-supervised-and-unsupervised-image-classification/"}
knitr::include_graphics('https://i0.wp.com/beekangsi.com/wp-content/uploads/2022/04/Supervised-and-unsupervised-classification-of-SPOT-5-image-of-the-study-area-1.jpg?fit=738%2C613&ssl=1&w=640')
```

Referencing to [@bekan2022]'s GIS blog about the pros and cons of supervised and unsupervised image classification, both classification techniques have their advantages and disadvantages, it is hard to determine which technique is better as it depends on the focus with the results. The

+---------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+
|                                 | Advantage                                                                                        | Disadvantage                                                                    |
+=================================+==================================================================================================+=================================================================================+
| **Unsupervised Classification** | 1.  The operator can spot mistakes and often corrects them.                                      | 1.  gathering training for different class is very difficult and time consuming |
|                                 |                                                                                                  |                                                                                 |
|                                 | 2.  The analyst does not have to worry about matching categories on the final map to field data. | 2.  the unrepresented place in training data is difficult to recognize          |
|                                 |                                                                                                  |                                                                                 |
|                                 | 3.  The process is completely within the control of the analyzer.                                | 3.  needs very clear training process                                           |
|                                 |                                                                                                  |                                                                                 |
|                                 | 4.  Specific sections of known identity are linked to processing.                                | 4.  and also requires labelled data set                                         |
|                                 |                                                                                                  |                                                                                 |
|                                 | 5.  the class s defined by the Analyst                                                           |                                                                                 |
|                                 |                                                                                                  |                                                                                 |
|                                 | \                                                                                                |                                                                                 |
+---------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+
| **Supervised Classification**   | 1.  It is not necessary to label the training data set.                                          | 1.  Along the classification process, there is no concept of output.            |
|                                 |                                                                                                  |                                                                                 |
|                                 | 2.  it is time saving process                                                                    | 2.  It is not possible to estimate or map the outcome of a new sample.          |
|                                 |                                                                                                  |                                                                                 |
|                                 | 3.  fast classification                                                                          | 3.  In the presence of outliers, the outcome varies greatly.                    |
+---------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+

### **Object Based Image Analysis (OBIA)**

In the lecture, another type of image classification is taught, the **Object based image analysis (OBIA)**. It involves pixels first being grouped into objects based on either spectral similarity or an external variable such as ownership, land use, shape and geological unit. **Superpixels** in OBIA are segmentating an image into regions by considering similarity(homogenity) or difference(heterogeneity) measures defined using perceptual features. To achieve that, Simple Linear Iterative Clustering (SLIC) algorithm is commonly used to generate Superpixels. SLIC uses euclidean distance to work out spatial distance between points to centre of pixels and is an adapted k means clustering. Thereby, it does not consider connectivity and do not compare each pixels with all pixels in the scene. Different from SLIC, another version of SLIC that is parameter-free has been proposed, called SLICO. It generates regular shaped Superpixels across the scene, regardless of textured or non-textured regions in the image.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="SLIC and SLICO Superpixel Generation. Source: https://www.mdpi.com/2072-4292/9/3/243"}
knitr::include_graphics('https://www.mdpi.com/remotesensing/remotesensing-09-00243/article_deploy/html/images/remotesensing-09-00243-g004-550.jpg')
```

### Sub-Pixel Analysis

Sub pixel classification (SPC) extracts meaningful information on land-cover classes from the mixed pixels. The figure below shows what a mixed pixel looks like. The proportion and abundance of each classes in the mixed pixel can be accounted for. In general, a few endmembers that are specturally pure are generated will work out fractions per pixel and establish a spectral library.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Sparse modeling at sub-pixel level using the pre-learned dictionary. Source: https://www.mdpi.com/2072-4292/13/2/190"}
knitr::include_graphics('https://www.mdpi.com/remotesensing/remotesensing-13-00190/article_deploy/html/images/remotesensing-13-00190-g004-550.jpg')

```

According to [@powell2007], for multiple endmember, Spectral mixture analysis (SMA) is done based on the assumptionthat the reflectance P' measured at pixel i can be modeled as the linear sum of N endmembers, or spectrally 'pure' materials, weighted by the fraction cover fki of each endmember within the field of view of pixel i. Below shows the calculation of the sum of end member reflectance franction contribution to best-fit mixed spectrum where eiÎ» is a residual term indicating the disagreement between the measured and modeled spectra.:

$$
p_i\lambda=\sum_{k=1}^{n} (p_{ki\lambda} * f_ki) + ei_\lambda
$$

By the end, there will be the percentage/fraction of components for each pixels, for example, fraction of vegetation, soil, urban areas.

### GEE practice with Shiga

To actually understand how classification works, in this week's practical, I have chosen Shiga, a prefecture of Japan that is located in the Kansai Region of Honshu. I have used Landsat data and vector data of area boundary. As the location has a very big lake, I thought it would show clear result for at least water and land, therefore I chose this as my area of study.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Shiga"}
knitr::include_graphics('figures/week7_japanshiga.png')

```

#### Sup-pixel analysis in Shiga

To perform sub-pixel analysis, I have defined variables as endmembers, then using `unmix()` function in GEE to add up the data and generate image. Similar to previous practicals, I have to filter the data with lower cloud coverage and within a time frame. But for the limited data available in the area, I adjusted the date from 2019-01-01 to 2023-3-10. After masking the cloud cover, the image is clipped to the boundary.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Shiga"}
knitr::include_graphics('figures/week7_tci.png')

```

Instead of inserting the coordinate values for the endmembers, I used the geometry function to generate a list of values for each category and extract the mean. As I have 6 categories, I did not want to redo everything manually, therefore I followed the practical to make the process of extracting mean value to a function and applied to the other landcover data. Finally, using `unmix()` on the processed data to create a map. However, as the map shown is unconstrained, the don't sum up to 1, hence another function of `ee.Image.unmix()` is used to compute the pseudo-inverse and multiplying through each pixel, then the image will return with positive values.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Sub-pixel and fixing th unconstrained problem"}
knitr::include_graphics('figures/week7_unmix.gif')

```

The main concern with the result above is the generation of error matric, hence there will be the part of hardening the sub-pixel image and classify each pixel that has the largest proportion of land cover. A chunk of code that manually assign value to each category of data with a threshold is done, for example, pixels that are greater than 0.5 will be assigned as 1 and if it is 0.5or below, it will be assigned as 0. After all the computation to reclassify each class, the different class of data needs to be added together. And the result is:

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Reclassified sub-pixel result"}
knitr::include_graphics('figures/week7_reclassified.png')

```

Just from the figure above, I can't interpret or explain if the analysis is accurate or not, I am not sure ifin GEE, there are some functions to view or reveal the statistics of the analysis. But I think for now I can't think of a way to interpret the results, such as explaining the accuracy, error etc.

#### Object based image analysis (OBIA)

From the previous summary about OBIA, it involves pixels first being grouped into objects based on either spectral similarity. In other words, pixels are grouped together based on some set of rules. To achieve that in the practical, the Image gradient is created from a function of `.gradient()` to change the intensity and colour in the image. Then, to make it useful for further steps of the analysis, the image gradient has to change to spectral gradient such that it is valuable to the analysis. It is done by `.spectralGradient().`

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Spectral Gradient in Shiga"}
knitr::include_graphics('figures/week7_spectralgradient.png')
```

Then, the next obstacle to carry out OBIA is that the pixels are too small and fragmented, there is a need to reduce pixels to fewer objects, hence the k-means clustering is used to reduce pixels.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="kmeans clustering"}
knitr::include_graphics('figures/week7_kmeans_seed.png')

```

Another option to reduce pixels and create super pixel is the Simple non-iterative clustering (SNIC) by making a **seed grid** that specifies the spacing in pixels and set a square or a hex grid.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Reclassified sub-pixel result"}
knitr::include_graphics('figures/week7_seed.png')
```

After that, the clusters will allow some useful information to be extracted, such as NDVI...

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Reclassified sub-pixel result"}
  knitr::include_graphics('figures/week7_NDVI.png')
```

However, when I try to apply classification to the data, it did not work due to a layer that I could not get rid of, it said"(7 bands for 1685346 pixels = 90.0 MiB \> 80.0 MiB). If this is a reduction, try specifying a larger 'tileScale' parameter."I have tried to make some of the maps as a note with "//" to make my code less heavy, but it didn't work. I have also tried to set higher tileScale, it also does not work. I tried to find solutions online but the answers on stack overflow don't seem to match my problem. Perhaps the region that I use is still too big, hence the computing is overloaded. It's a shame that I could not report results here, but at the same time I would love to find a solution to resolve this problem someday.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Layer Error for classification output"}
knitr::include_graphics('figures/WEEK7_ERROR.png')
```

## Application

The methods mentioned in this lecture are quite advanced and pertinent to reality such as OBIA considers objects instead of pixels. I find this interesting at first because OBIA images look like pixelate images. But after learning about the principle of OBIA, I think it is an effective tool for detecting challenging objects such as animals that can move. In a study by @chabot2018, the study aims to detect and count birds in large volumes of aerial imagery using OBIA software. their approach was to select training inmages and manually identify birds first, then establishing image segmentation parameters, including features boundaries, segmentation. They transform the input image into contrast gradient map, where higher values are assigned to pixels along the boundaries of spectrally contrasting features such as Snow geese against a darker ground. After the pre-processing, they compile attribute data of segmented bird objects and set up bird object classification rule sets

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap=" automated object-based analysis of large volumes of aerial imagery to detect Lesser Snow Geese. Source: https://www.ace-eco.org/vol13/iss1/art15/#Compiling"}
knitr::include_graphics('https://www.ace-eco.org/vol13/iss1/art15/figure9.jpg')
```

I think this study design is very comprehensive and almost flawless as it accounts for most of the potential errors and how to mitigate them. Also, the description and selection of the raw image used is well accounted for. The steps to set up parameters for bird identification and procedure for OBIA are very detailed. I think the project is very interesting because birds are moving objects that are constantly moving. Hence, I wonder if there will be overlapped count for the same bird because it is not possible to identify each individual birds in a remotely sensed image.

## Reflection

In this week, I have learnt quite in-depth about image classification in remote sensing. I think different analysis has its own advantage, for example, the sub-pixel analysis takes into account the possibility of a pixel to belong to different classes in an image segmentation context.The mixed pixels makes image classification very precise and high resolution. this has enhanced the quality of output in remote sensing and also helps to broaden the capability of remote sensing in research that require higher resolution images, such as animal studies, deep learning etc.

Also with the GEE, I think the OBIA is quite easy to compute as it is only a few lines of codes from reducing pixels through kmeans or create hex or square grids to running a cluster function. In the future, I think I would need more experience with running machine learning model in GEE because the training and testing data split is still complicated for me. Thus, the classification part has been quite confusing to me on how it is actually executed.
