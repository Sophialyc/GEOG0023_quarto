---
title: "Week 6"
---

<div style="text-align: justify">

## Summary of Lecture 6

This is the second lecture that covers Google Earth Engine (GEE), and it is more focused on image classification. Before going deep into what image classification refers to in technical terms, this lecture focused on how classified images are used in different contexts to solve various problems, also introducing machine learning in remote sensing. To briefly understand the term image classification, it refers to the process of identifying and categorising images according to their appearances or pixels, it is an imperative process before actually analyzing them in research or studies because the classified images provides concrete information of the classes captured in an image.

### Utilising classified image in different scenarios

#### Urban Expansion

Urban land use chages have long been a hot topic for the research field, the incoporation of satellite images and GIS has generated a new dimension for researchers to assess and monitor land use cover change [@tewolde2011]. But the raw images captured from remote sensors are not easy to use as they do not convey any useful information for analysis, hence, studies that use satellite images for urban land use changes area often classified, creating a scale of values to compare and use.

For instance, in a study on Eritrea, an independent state in north-eastern Africa, its capital city Asmara was examined with its settlement patterns of urban and suburban areas. Images used were Landsat TM satellite images with 30m resolution from 1908, 2000 and 2009. Processing steps of landsat image is shown below:

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Data and Methodology used for image classification and result validation. Source: https://www.mdpi.com/2072-4292/3/10/2148"}
knitr::include_graphics('https://www.mdpi.com/remotesensing/remotesensing-03-02148/article_deploy/html/images/remotesensing-03-02148-g002-1024.png')
```

Object-Based Image Analysis (OBIA) was adopted to classify images at image object level instead of pixel level. For land use study, OBIA is useful because the real world is not made up of pixels, instead, it is arrnaged in objects. Therefore, OBIA can avoid mixed pixel problems such as bare sand soil and the impervious parts of urban areas could create a mixed pixel problem. Hence, with classified images, it can be put to a model for analysis in urban expansion.

#### Air Pollution and LULC

Classified images are useful for identifying air pollution and monitoring variations. For instance, images with classified land use and land cover (LULC) can combine with air quality data, such as the type of air pollutiants, concentration of pollutant particles to see where in the image of an area is prone to air pollution. To illustrate, a study by [@superczynski2011] has explored LULC and Pm~2.5~ emissions. The classified images to LULC map was linked to Pm~2.5~ mass concentration in Alabama to examine the correlation between the two variables. Classification was done with selecting representative samples for each class, which is a supervised classification. It was mentioned that some calssed in the image has to be split into subclasses and combined into Level I Anderson classification in post-classification processing. The figure below shows the result of classification to urban areas in Birmingham in Alabama. Then, the data was used to compute with Pm~2.5~ data for further air pollution analysis.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="progression of urbanized land around the city of Birmingham since 1998. Source: https://www.mdpi.com/2072-4292/3/12/2552"}
knitr::include_graphics('https://www.mdpi.com/remotesensing/remotesensing-03-02552/article_deploy/html/images/remotesensing-03-02552-g002-1024.png')
```

### Machine learning in remote sensing with GEE

Machine learning has been applied in remote sensing for various usage, including image classification, regression, clustering, coding and source separation. In the lecture, **Classification and regression trees (CART)** was introduced. The two types of decision trees are:

-   Classification trees (CA) : classify data into **≥** 2 discrete categories

    -   examples: temperature. rainfall, wind, saturation

-   Regression trees (RT) : predict continuous dependable variables

    -   examples: age, height, score

#### Classification trees

The CART algorithm was published by Leo Breiman in 1984, using decision tree to provide more information to learn from in the form of basic if-else-decision rules. The decision tree is a simple structure that consist of different kinds of elements, a **starting point (Root)**, **decision nodes** and **multiple terminal nodes (Leaves)**. Eventually, each pixel points will be assigned to class for classification purpose. In other words, classification trees tries to subset data into smaller chunks, it is useful when linear regression does not fit, by subseting the original data, the classification tree can generate different category.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Knowledge-based decision tree. Source: https://www.mdpi.com/2072-4292/4/6/1741"}
knitr::include_graphics('https://www.mdpi.com/remotesensing/remotesensing-04-01741/article_deploy/html/images/remotesensing-04-01741f8-1024.png')
```

The concept of decision tree is easily grasped, hence, it is rather easy to employ in different analysis. Also, as it can be visualised, people that works better with visualisation in problem solving will benefit from it. However, on the downside, decision tree are prone to **overfitting,** causing more complex trees fail to properly generalise data. Also, there is the possibility that the final leaves result with a mixture of different categories --\> **Impurity.**

To solve the impurity for clear classification, the **Gini Impurity** is quantified and used as splitting methods, where it measures the likelihood of an incorrect classification of a new instance of a random variable. The Gini Impurity formula is calculated as:

$$
G=i=1∑C​p(i)∗(1−p(i))
$$

Two other common impurity metrics are **Entropy:**

$$
Ie(S) = −∑pilog2(pi) 
$$

and **Missclassifcation error:**

$$
Ic(S) =1 – max(pi)
$$

Gini index and entropy behave similarly,both attempting to reach a unique maximum, though at slightly different positions. Gini Impurity of 0 is the lowest and best possible impurity, but it only occurs when all the pixels are in the same class. While for misclassification error, it is simply the fraction of points in the subset that are not in the major vote class, it's more of pruning nature and not growing a tree.

#### Regression trees

Different from classification trees, regression trees are used when the response variable is [continuous]{.underline}. With many predictor variables, the decision trees can undergo trials of different thresholds and calculate the sum of squared residuals (SSR). It helps to predict continuous variables such as amount of pollution. The figure below shows major difference of regression tree and classification tree.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Classifcation vs Regression . Source: https://www.javatpoint.com/regression-vs-classification-in-machine-learning"}
knitr::include_graphics('https://static.javatpoint.com/tutorial/machine-learning/images/regression-vs-classification-in-machine-learning.png')
```

The normal workflow for a regression tree is in the first split, compare and find the lowest average value, then consider another split, find the lowest value again and so on.

#### Random Forest 

In classification of new data, using one decision tree is not ideal, because it is not that accountable. However, with many trees, that form a forest, there are more decision trees to increase accuracy and accountability. In addition to our data, boostrap samples are taken, then create a decision tree from random number of variables.

```{r echo=FALSE, cache=FALSE, out.width="100%", fig.align='center', fig.cap="Random forest classifier . Source: https://www.freecodecamp.org/news/how-to-use-the-tree-based-algorithm-for-machine-learning/"}
knitr::include_graphics('https://www.freecodecamp.org/news/content/images/2020/08/how-random-forest-classifier-work.PNG')
```

It is a tree-based supervised learning algorithms by combining hundreds decision trees and train each tree on a different sample of the observations. **Bootstrapping** is a resampling method by replacing data to make a decision, which is called **Bagging**. So what exactly does the two terms mean?

**Bootstrapping** : A resampling technique that involves random sampling of a dataset *with replacement;* allowing generation of new samples from a population without having to go and collect additional "training data". However, a major drawback of decision trees is being *high-variance estimators,* meaning that the addition of small number of training observations could alter the prediction of a learned tree. Therefore, bagging or bootstrap aggregation can be used to mitigate the problem

**Bagging**: Combine multiple learners that have fitted on separate bootstrapped samples and average the predictions. This effectively reduce overall variance of the predictions. In random forests, bagging helps to decrease the correlation between each decision trees and thus increasing its predictive accuracy.\

## Application

In this practical, I chose Antwerpen to try and apply with the aforementioned classifications and machine learning in GEE. Instead of uploading the shapefile, the GAUL global admin layers and level 2 in GEE's catalog was used. Antwerpen is filtered out. Using Sentinel 2 MSI level 2A , I filtered the entire collection based on a low cloud coverage percentage of 5% per tile within the time period of 2022-01-01 to 2022-10-31. Below shows the gif the clipped image of ANtwerpen by increasing the cloud coverage, but using the masking function to mask the cloud pixels.

## Reflection
